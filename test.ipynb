{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.RMP.RMPControllers import *\n",
    "from src.RMP.RMPControllers.RMP_standing import rmp_standing\n",
    "from src.RMP.RMPControllers.RMP_constraint import RMP_joint_limits\n",
    "from src.RMP.RMP import combine_rmp\n",
    "from src.AEncoder import scalar_embedding\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_samples):\n",
    "            data = []\n",
    "            for _ in range(num_samples):\n",
    "                r = np.random.rand()\n",
    "                if r < 0.5:\n",
    "                    data.append(np.random.uniform(-1, 1, 3))\n",
    "                elif r < 0.75:\n",
    "                    data.append(np.random.uniform(-10, 10, 3))\n",
    "                else:\n",
    "                    data.append(np.random.uniform(-100, 100, 3))\n",
    "            return torch.tensor(np.array(data, dtype=np.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = scalar_embedding.ScalarEmbedder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs=50, n_samples=1_000):\n",
    "    for epoch in range(n_epochs):\n",
    "        x = generate_data(n_samples)\n",
    "        for i in range(n_samples):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            reconstructed, scalar_embedding = model(x[i])\n",
    "            \n",
    "            loss = torch.sqrt(criterion(reconstructed, x[i]))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('Epoch: ', epoch, 'Loss: ', loss.item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  6.570186138153076\n",
      "Epoch:  1 Loss:  6.573554992675781\n",
      "Epoch:  2 Loss:  59.17803955078125\n",
      "Epoch:  3 Loss:  56.63607406616211\n",
      "Epoch:  4 Loss:  0.15152746438980103\n",
      "Epoch:  5 Loss:  7.579577922821045\n",
      "Epoch:  6 Loss:  36.091312408447266\n",
      "Epoch:  7 Loss:  0.7729549407958984\n",
      "Epoch:  8 Loss:  73.85183715820312\n",
      "Epoch:  9 Loss:  0.7330651879310608\n",
      "Epoch:  10 Loss:  6.293079853057861\n",
      "Epoch:  11 Loss:  0.43664678931236267\n",
      "Epoch:  12 Loss:  0.28313887119293213\n",
      "Epoch:  13 Loss:  0.6600363254547119\n",
      "Epoch:  14 Loss:  0.661529004573822\n",
      "Epoch:  15 Loss:  5.380307674407959\n",
      "Epoch:  16 Loss:  0.592415452003479\n",
      "Epoch:  17 Loss:  57.44302749633789\n",
      "Epoch:  18 Loss:  3.5751595497131348\n",
      "Epoch:  19 Loss:  60.11357498168945\n",
      "Epoch:  20 Loss:  3.8370258808135986\n",
      "Epoch:  21 Loss:  7.300537586212158\n",
      "Epoch:  22 Loss:  52.3853645324707\n",
      "Epoch:  23 Loss:  0.8013611435890198\n",
      "Epoch:  24 Loss:  0.09013502299785614\n",
      "Epoch:  25 Loss:  0.30805644392967224\n",
      "Epoch:  26 Loss:  0.34129199385643005\n",
      "Epoch:  27 Loss:  0.3343479037284851\n",
      "Epoch:  28 Loss:  66.35167694091797\n",
      "Epoch:  29 Loss:  5.4737067222595215\n",
      "Epoch:  30 Loss:  52.68631362915039\n",
      "Epoch:  31 Loss:  5.195643424987793\n",
      "Epoch:  32 Loss:  7.150909423828125\n",
      "Epoch:  33 Loss:  0.35653984546661377\n",
      "Epoch:  34 Loss:  0.6371610164642334\n",
      "Epoch:  35 Loss:  0.48724836111068726\n",
      "Epoch:  36 Loss:  0.6538349986076355\n",
      "Epoch:  37 Loss:  0.3484605550765991\n",
      "Epoch:  38 Loss:  6.74327278137207\n",
      "Epoch:  39 Loss:  78.14741516113281\n",
      "Epoch:  40 Loss:  0.2869340479373932\n",
      "Epoch:  41 Loss:  0.41829240322113037\n",
      "Epoch:  42 Loss:  0.6663683652877808\n",
      "Epoch:  43 Loss:  0.6485092639923096\n",
      "Epoch:  44 Loss:  0.631519079208374\n",
      "Epoch:  45 Loss:  30.3006534576416\n",
      "Epoch:  46 Loss:  0.7631039619445801\n",
      "Epoch:  47 Loss:  52.04727554321289\n",
      "Epoch:  48 Loss:  3.3297805786132812\n",
      "Epoch:  49 Loss:  0.4431023895740509\n",
      "Epoch:  50 Loss:  0.3658088147640228\n",
      "Epoch:  51 Loss:  5.663427829742432\n",
      "Epoch:  52 Loss:  7.020504951477051\n",
      "Epoch:  53 Loss:  0.5710805654525757\n",
      "Epoch:  54 Loss:  0.3739232122898102\n",
      "Epoch:  55 Loss:  28.25242805480957\n",
      "Epoch:  56 Loss:  0.23226414620876312\n",
      "Epoch:  57 Loss:  4.229287147521973\n",
      "Epoch:  58 Loss:  0.3991304934024811\n",
      "Epoch:  59 Loss:  5.522082805633545\n",
      "Epoch:  60 Loss:  7.422144412994385\n",
      "Epoch:  61 Loss:  78.57038879394531\n",
      "Epoch:  62 Loss:  0.9762927889823914\n",
      "Epoch:  63 Loss:  3.6339869499206543\n",
      "Epoch:  64 Loss:  26.749839782714844\n",
      "Epoch:  65 Loss:  5.046659469604492\n",
      "Epoch:  66 Loss:  4.72829532623291\n",
      "Epoch:  67 Loss:  0.10636438429355621\n",
      "Epoch:  68 Loss:  16.519052505493164\n",
      "Epoch:  69 Loss:  0.8487825393676758\n",
      "Epoch:  70 Loss:  0.22255036234855652\n",
      "Epoch:  71 Loss:  0.5350304245948792\n",
      "Epoch:  72 Loss:  0.4894404411315918\n",
      "Epoch:  73 Loss:  86.53514862060547\n",
      "Epoch:  74 Loss:  0.3142746388912201\n",
      "Epoch:  75 Loss:  0.5226681232452393\n",
      "Epoch:  76 Loss:  0.6578701734542847\n",
      "Epoch:  77 Loss:  44.913089752197266\n",
      "Epoch:  78 Loss:  0.6296655535697937\n",
      "Epoch:  79 Loss:  0.5688177943229675\n",
      "Epoch:  80 Loss:  5.446363925933838\n",
      "Epoch:  81 Loss:  0.8118169903755188\n",
      "Epoch:  82 Loss:  0.6670787930488586\n",
      "Epoch:  83 Loss:  7.913029670715332\n",
      "Epoch:  84 Loss:  57.01970291137695\n",
      "Epoch:  85 Loss:  5.613052845001221\n",
      "Epoch:  86 Loss:  62.983375549316406\n",
      "Epoch:  87 Loss:  6.014878273010254\n",
      "Epoch:  88 Loss:  76.48951721191406\n",
      "Epoch:  89 Loss:  0.5482386946678162\n",
      "Epoch:  90 Loss:  59.737571716308594\n",
      "Epoch:  91 Loss:  0.6013071537017822\n",
      "Epoch:  92 Loss:  5.741423606872559\n",
      "Epoch:  93 Loss:  66.30653381347656\n",
      "Epoch:  94 Loss:  0.5352828502655029\n",
      "Epoch:  95 Loss:  0.8527217507362366\n",
      "Epoch:  96 Loss:  0.5874270796775818\n",
      "Epoch:  97 Loss:  0.5121060609817505\n",
      "Epoch:  98 Loss:  7.684940814971924\n",
      "Epoch:  99 Loss:  0.5375279188156128\n"
     ]
    }
   ],
   "source": [
    "train(n_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.1008e+02],\n",
       "        [3.1746e-01],\n",
       "        [3.6378e-01]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[10.5, 0.3, 0.8], [0.2, 0.9, 0.4], [0.1, 0.5, 0.6]], dtype=torch.float32)\n",
    "reconstructed, scalar_embedding = model(x)\n",
    "scalar_embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
